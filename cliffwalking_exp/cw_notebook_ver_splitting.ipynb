{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sys\n",
    "# if \"../\" not in sys.path:\n",
    "#   sys.path.append(\"../\") \n",
    "from collections import defaultdict\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "from scipy.optimize import Bounds\n",
    "from tqdm import tqdm\n",
    "bounds = Bounds([-0.1,-0.1],[0.1,0.1])\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "\n",
    "# download from https://github.com/Ma-sa-ue/OPE_codes?files=1\n",
    "Q_space = np.load(\"Q-table-cliff.npz\")[\"xxx\"]\n",
    "Q_space2 = np.load(\"Q-table-cliff.npz\")[\"xxx\"]\n",
    "\n",
    "prob1 = [1.0 for i in range((env.nA))]\n",
    "prob1 = prob1/np.sum(prob1)\n",
    "\n",
    "###################\n",
    "## discount rate ##\n",
    "###################\n",
    "\n",
    "\n",
    "gamma = 0.5\n",
    "\n",
    "if gamma == 0.7:\n",
    "    est_mean = -3.301627552777223\n",
    "elif gamma == 0.5:\n",
    "    est_mean = -2.003425807974581\n",
    "elif gamma == 0.45:\n",
    "    est_mean = -1.816693355290161\n",
    "elif gamma == 0.3:\n",
    "    est_mean = -1.4267969764571544\n",
    "###################################\n",
    "## parameter for behavior policy ## larger --> more expert \n",
    "###################################\n",
    "betabeta = 0.5\n",
    "#betabeta = 0.8\n",
    "#################################\n",
    "## parameter for target policy ##\n",
    "#################################\n",
    "alpha = 1\n",
    "#alpha = 0.9\n",
    "#################################\n",
    "## parameter for MC repetition ##\n",
    "#################################\n",
    "\n",
    "def sample_policy(observation,alpha=alpha):\n",
    "    prob2 = alpha*Q_space[observation,:] +(1-alpha)*prob1\n",
    "    return np.random.choice(env.nA,1,p=prob2)[0] ## 4个action 选一\n",
    "        \n",
    "def behavior_policy(observation,beta=betabeta):\n",
    "    prob2 = beta*Q_space[observation,:]+ (1-beta)*prob1 ## behavior这么聪明？\n",
    "    return np.random.choice(env.nA,1,p=prob2)[0]\n",
    "\n",
    "def target_dense(observation,alpha=alpha):\n",
    "    prob2 = alpha*Q_space[observation,:]+ (1-alpha)*prob1\n",
    "    return prob2\n",
    "\n",
    "def behav_dense(observation,beta=betabeta):\n",
    "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
    "    return prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa2(env, policy, policy2, num_episodes, discount_factor=1.0, Q_space2=Q_space2, alpha= 0.6, epsilon=0.03):\n",
    "    \"\"\"\n",
    "    policy : Nothing..\n",
    "    policy2 : Behavior policy\n",
    "    \"\"\"\n",
    "    print(\"doing SARSA to get Q prediction\")\n",
    "    \n",
    "    Q = np.zeros_like(Q_space2)\n",
    "    episode_episode = []\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        action = policy2(state)\n",
    "        episode = []\n",
    "        for t in itertools.count():\n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            # Pick the next action\n",
    "            next_action= policy2(next_state)\n",
    "            # TD Update\n",
    "            td_target = reward + discount_factor * np.sum(Q[next_state,:]*target_dense(next_state)) ## 这个就是 E (Q^\\pi)\n",
    "            td_delta = td_target - Q[state,action]\n",
    "            Q[state,action] += alpha * td_delta  ## TD update\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            action = next_action\n",
    "            state = next_state \n",
    "        episode_episode.append(episode)\n",
    "    \n",
    "    return Q, episode_episode ## return Q and buffers(trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/20000 [00:00<01:39, 199.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing SARSA to get Q prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:07<00:00, 297.34it/s]\n",
      "  0%|          | 76/20000 [00:00<00:26, 758.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:22<00:00, 907.67it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 548.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def mse(aaa, true = -3.301610434961245):\n",
    "    aaa = np.array(aaa)\n",
    "    aaa = aaa[aaa>-100]\n",
    "    return [np.mean((((aaa-true)*(aaa-true)))),np.sqrt(np.var((aaa-true)*(aaa-true)))]\n",
    "\n",
    "\n",
    "rep = 40\n",
    "\n",
    "sample_size = 1000 * rep\n",
    "sample_size = int(sample_size/2)\n",
    "\n",
    "\n",
    "predicted_Q ,episode_episode = sarsa2(env, \n",
    "                                      sample_policy,\n",
    "                                      behavior_policy,\n",
    "                                      sample_size, \n",
    "                                      discount_factor = gamma)\n",
    "\n",
    "env = env\n",
    "\n",
    "policy = sample_policy\n",
    "policy2 = behavior_policy\n",
    "episode_episode = episode_episode\n",
    "Q_= predicted_Q\n",
    "num_episodes= sample_size\n",
    "discount_factor=gamma\n",
    "\n",
    "\n",
    "depth = 1\n",
    "\n",
    "\n",
    "returns_sum = defaultdict(float)\n",
    "returns_count = defaultdict(float)\n",
    "returns_count2 = defaultdict(float)\n",
    "predic_list = []\n",
    "predic_list2 = []\n",
    "predic_list3 = []\n",
    "predic_list22 = []\n",
    "predic_list4 = []\n",
    "predic_list5 = np.ones(num_episodes)\n",
    "auxiauxi = [] \n",
    "epiepi = []\n",
    "weight_list = np.zeros([num_episodes,1000]) ### For bounded IPW\n",
    "weight_list2 = np.zeros([num_episodes,1002]) ### For bounded IPW\n",
    "weight_list3 = np.zeros([num_episodes,1002]) ### For bounded IPW\n",
    "marginal_weight = np.zeros([num_episodes,1000]) ### For bounded IPW\n",
    "marginal_weight_2 = np.zeros([num_episodes,1000]) ### For bounded IPW\n",
    "auxi_list = np.zeros([num_episodes,1000])\n",
    "marginal_auxi_list2 = np.zeros([num_episodes,1000])\n",
    "marginal_auxi_list = np.zeros([num_episodes,1000])\n",
    "marginal_auxi_list2_2 = np.zeros([num_episodes,1000])\n",
    "marginal_auxi_list_2 = np.zeros([num_episodes,1000])\n",
    "auxi_list2 = np.zeros([num_episodes,1000])\n",
    "reward_list = np.zeros([num_episodes,1000])\n",
    "state_list = np.zeros([num_episodes,1000])\n",
    "action_list = np.zeros([num_episodes,1000])\n",
    "\n",
    "count_list = np.zeros(1000) \n",
    "episolode_longe_list = []\n",
    "\n",
    "print(\"doing prediction\")\n",
    "info = []\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    episode = episode_episode[i_episode] ## extract episode\n",
    "    W = 1.0\n",
    "    W_list = []\n",
    "    episolode_longe_list.append(len(episode))\n",
    "    weight_list2[i_episode, 0] = 1.0\n",
    "    for t in range(len(episode)):\n",
    "        state, action, reward = episode[t]\n",
    "        reward_list[i_episode,t] = reward\n",
    "        state_list[i_episode,t] = state\n",
    "        action_list[i_episode,t] = action\n",
    "        if t == 0:\n",
    "            W = W * target_dense(state)[action]/behav_dense(state)[action]\n",
    "        else:\n",
    "            W = W * target_dense(state)[action]/behav_dense(state)[action] * discount_factor\n",
    "\n",
    "        probprob = alpha * Q_space[state,:] + (1 - alpha) * prob1\n",
    "\n",
    "        W_list.append(W)\n",
    "        weight_list[i_episode, t] = W_list[t]\n",
    "        weight_list2[i_episode, t+1] = W_list[t]\n",
    "        weight_list3[i_episode, t] = target_dense(state)[action]/behav_dense(state)[action]\n",
    "\n",
    "        count_list[t] += 1.0\n",
    "\n",
    "        if t==0:\n",
    "            auxi_list[i_episode,t] = W_list[t] * Q_[state,action] - np.sum(probprob * Q_[state,:])\n",
    "        else:\n",
    "            auxi_list[i_episode,t] = W_list[t] * Q_[state,action]- W_list[t-1] * np.sum(probprob * Q_[state,:])\n",
    "\n",
    "        if t==0:\n",
    "            auxi_list2[i_episode,t] = W_list[t]-1.0\n",
    "        else:\n",
    "            auxi_list2[i_episode,t] = W_list[t]-W_list[t-1]\n",
    "\n",
    "# print np.max(np.array(episolode_longe_list))\\\n",
    "# print(episolode_longe_list)\n",
    "# print(np.array(episolode_longe_list))\n",
    "\n",
    "\n",
    "weight_list_mean = np.mean(weight_list,1)\n",
    "reward_list_mean = np.mean(reward_list,1)\n",
    "auxi_list_mean = np.mean(auxi_list,1)\n",
    "auxi_list2_mean = np.mean(auxi_list2,1)\n",
    "\n",
    "val = []    \n",
    "\n",
    "##### IPW\n",
    "for i in range(num_episodes):\n",
    "    predic_list.append(np.sum(weight_list[i,:]*reward_list[i,:]))   \n",
    "val.append(np.mean(predic_list)) ## val[0]\n",
    "\n",
    "#### Marginalized-IPW \n",
    "\n",
    "for i in range(num_episodes):\n",
    "    for j in range(episolode_longe_list[i]):\n",
    "        marginal_weight[i,j] = np.mean(weight_list[:,j][(state_list[:,j]==state_list[i,j]) \n",
    "                                                        & (action_list[:,j]==action_list[i,j])])\n",
    "        if j == 0:\n",
    "            marginal_weight_2[i,j] = weight_list3[i,j]\n",
    "        else:\n",
    "            marginal_weight_2[i,j] = np.mean(weight_list[:,j-1][(state_list[:,j]==state_list[i,j])])*weight_list3[i,j]\n",
    "\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    for t in range(episolode_longe_list[i_episode]):\n",
    "        state = np.int(state_list[i_episode,t])\n",
    "        action = np.int(action_list[i_episode,t])\n",
    "        probprob = alpha * Q_space[state,:] + (1 - alpha) * prob1\n",
    "        if t==0:\n",
    "            marginal_auxi_list[i_episode,t] = marginal_weight[i_episode,t] * Q_[state,action]- \\\n",
    "                                              np.sum(probprob*Q_[state,:])\n",
    "            marginal_auxi_list_2[i_episode,t] = marginal_weight_2[i_episode,t] * Q_[state,action]- \\\n",
    "                                                np.sum(probprob*Q_[state,:])\n",
    "            auxi_list[i_episode,t] = weight_list[i_episode,t]*Q_[state,action]-np.sum(probprob*Q_[state,:])\n",
    "        else:\n",
    "#             marginal_auxi_list[i_episode,t] = marginal_weight[i_episode,t]*(Q_[state,action])-marginal_weight[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "#             marginal_auxi_list_2[i_episode,t] = marginal_weight_2[i_episode,t]*(Q_[state,action])-marginal_weight_2[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "#             auxi_list[i_episode,t] = weight_list[i_episode,t]*(Q_[state,action])-weight_list[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "            marginal_auxi_list[i_episode,t] = marginal_weight[i_episode,t]*(Q_[state,action])- \\\n",
    "                                discount_factor * marginal_weight[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "            marginal_auxi_list_2[i_episode,t] = marginal_weight_2[i_episode,t]*(Q_[state,action])-\\\n",
    "                                discount_factor * marginal_weight_2[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "            auxi_list[i_episode,t] = weight_list[i_episode,t]*(Q_[state,action])- \\\n",
    "                                discount_factor * weight_list[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
    "\n",
    "        if t==0:\n",
    "            marginal_auxi_list2[i_episode,t] = marginal_weight[i_episode,t]-1.0\n",
    "            marginal_auxi_list2_2[i_episode,t] = marginal_weight_2[i_episode,t]-1.0\n",
    "            auxi_list2[i_episode,t] = weight_list[i_episode,t]-1.0\n",
    "        else:\n",
    "            marginal_auxi_list2[i_episode,t] =  marginal_weight[i_episode,t]- marginal_weight[i_episode,t-1]\n",
    "            marginal_auxi_list2_2[i_episode,t] =  marginal_weight_2[i_episode,t]- marginal_weight_2[i_episode,t-1]\n",
    "            auxi_list2[i_episode,t] = weight_list[i_episode,t]-weight_list[i_episode,t-1]\n",
    "\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    predic_list2.append(np.sum(marginal_weight[i,:]*reward_list[i,:]))   \n",
    "\n",
    "### marginal ipw2  #### Using action and state \n",
    "val.append(np.mean(predic_list2))  ## val[1]\n",
    "\n",
    "\n",
    "### marginal ipw3#### Using only state \n",
    "for i in range(num_episodes):\n",
    "    predic_list22.append(np.sum(marginal_weight_2[i,:]*reward_list[i,:]))    ## val[2]\n",
    "\n",
    "val.append(np.mean(predic_list22))\n",
    "\n",
    "\n",
    "#### DR ## check\n",
    "val.append(np.mean(predic_list) - np.mean(np.sum(auxi_list,1)))  # DRL(MDP)\n",
    "\n",
    "#### marginal DR 1  #### Using action and state \n",
    "val.append(np.mean(predic_list2) - np.mean(np.sum(marginal_auxi_list,1)))     # DRL(NMDP, lag=3)\n",
    "#### marginal DR 2   #### Using only state                                     \n",
    "val.append(np.mean(predic_list22) - np.mean(np.sum(marginal_auxi_list_2,1)))  # DRL(NMDP, lag=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wis_output: CI 0.975, AL 0.376, pred -2.012, MSE 0.0090(0.0109)\n",
      "ipw3_output: CI 0.000, AL 0.633, pred -3.016, MSE 1.0485(0.2889)\n",
      "dr_output: CI 1.000, AL 0.182, pred -2.011, MSE 0.0017(0.0016)\n",
      "dr2_output: CI 1.000, AL 0.182, pred -2.011, MSE 0.0017(0.0016)\n",
      "dr3_output: CI 0.000, AL 0.325, pred -2.629, MSE 0.3955(0.0819)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## output\n",
    "wis_output  = predic_list\n",
    "ipw3_output = predic_list22\n",
    "dr_output = predic_list - np.sum(auxi_list,1)\n",
    "dr2_output = predic_list2 - np.sum(marginal_auxi_list,1)\n",
    "dr3_output = predic_list22 - np.sum(marginal_auxi_list_2,1)\n",
    "\n",
    "for method, output in zip([\"wis_output\", \"ipw3_output\", \"dr_output\", \"dr2_output\", \"dr3_output\"], \n",
    "                          [wis_output, ipw3_output, dr_output, dr2_output, dr3_output]):\n",
    "    \n",
    "    count = 0\n",
    "    pred_output = []\n",
    "    AL = []\n",
    "    alpha = 0.05\n",
    "\n",
    "    for i in range(rep):\n",
    "        n = int(sample_size/rep)\n",
    "        V_output = output[i * n  : (i + 1) * n]\n",
    "        lower_bound = np.mean(V_output) - norm.ppf(1 - alpha/2) * np.std(V_output)/(n**0.5)\n",
    "        upper_bound = np.mean(V_output) + norm.ppf(1 - alpha/2) * np.std(V_output)/(n**0.5)\n",
    "        #lower_bound, upper_bound, upper_bound - lower_bound, len(V_output)\n",
    "        if lower_bound < est_mean and est_mean < upper_bound:\n",
    "            count += 1\n",
    "        AL.append(upper_bound - lower_bound)\n",
    "        pred_output.append((lower_bound + upper_bound) / 2)\n",
    "    print(\"%s: CI %.3f, AL %.3f, pred %.3f, MSE %.4f(%.4f)\" %(method, count / len(AL),  np.mean(AL), np.mean(pred_output), *mse(pred_output, est_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wis -1.9878388767460762 2.1234158255288036\n",
      "ipw3 -2.946639298358889 3.580992582049927\n",
      "dr -2.00367294979227 0.8599963829612072\n",
      "dr3 -1.7500946343754875 1.3391783883898343\n"
     ]
    }
   ],
   "source": [
    "print(\"wis\", np.mean(predic_list), np.std(predic_list))\n",
    "print(\"ipw3\", np.mean(predic_list22), np.std(predic_list22))\n",
    "print(\"dr\", np.mean(predic_list - np.sum(auxi_list,1)), np.std(predic_list - np.sum(auxi_list,1)))\n",
    "print(\"dr3\", np.mean(predic_list22 - np.sum(marginal_auxi_list_2,1)), np.std(predic_list22 - np.sum(marginal_auxi_list_2,1)))\n",
    "\n",
    "wis_output  = predic_list\n",
    "ipw3_output = predic_list22\n",
    "dr_output = predic_list - np.sum(auxi_list,1)\n",
    "dr3_output = predic_list22 - np.sum(marginal_auxi_list_2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr3 [0.16089610591722953, 1.1053714990773462]\n",
      "dr [0.08631137838797087, 0.5508526551595504]\n",
      "ipw3 [53.997189444281005, 277.1743438097648]\n",
      "wis [27.35327656291926, 131.55643170999835]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mse(aaa, true = -3.301610434961245):\n",
    "    aaa = np.array(aaa)\n",
    "    aaa = aaa[aaa>-100]\n",
    "    return [np.mean((((aaa-true)*(aaa-true)))),np.sqrt(np.var((aaa-true)*(aaa-true)))]\n",
    "\n",
    "print(\"dr3\", mse(dr3_output))\n",
    "print(\"dr\", mse(dr_output))\n",
    "print(\"ipw3\", mse(ipw3_output))\n",
    "print(\"wis\", mse(wis_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.329633795201816, -3.259327011705076, 0.07030678349673991, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "alpha = 0.05\n",
    "n = sample_size\n",
    "V_output = dr3_output\n",
    "lower_bound = np.mean(V_output) - norm.ppf(1 - alpha/2) * np.std(V_output)/(n**0.5)\n",
    "upper_bound = np.mean(V_output) + norm.ppf(1 - alpha/2) * np.std(V_output)/(n**0.5)\n",
    "lower_bound, upper_bound, upper_bound - lower_bound, len(V_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true = -42.49\n",
    "# def tablize_result(aaa):\n",
    "#     aaa = np.array(aaa)\n",
    "#     # aaa = aaa[aaa>-100]\n",
    "#     rmse_value = np.sqrt(np.mean(np.square(aaa - true)))\n",
    "#     # sd_value = np.std(np.square(aaa - true))\n",
    "#     sd_value = np.std(aaa - true)\n",
    "#     return [rmse_value, sd_value]\n",
    "\n",
    "# print(\"is\")\n",
    "# print(tablize_result(is_list))\n",
    "# print(\"dr3\")\n",
    "# print(tablize_result(dr3_list))\n",
    "# print(\"dm\")\n",
    "# print(tablize_result(dm_list))\n",
    "# print(\"mis\")\n",
    "# print(tablize_result(is3_list))\n",
    "# print(\"dr\")\n",
    "# print(tablize_result(dr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.202400967458164]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = -42.58326666666667\n",
    "def mse(aaa):\n",
    "    aaa = np.array(aaa)\n",
    "    aaa = aaa[aaa>-100]\n",
    "    return [np.mean((((aaa-true)*(aaa-true)))),np.sqrt(np.var((aaa-true)*(aaa-true)))]\n",
    "\n",
    "print(\"is\")\n",
    "print(np.mean(is_list))\n",
    "print(mse(is_list))\n",
    "print(\"wis\")\n",
    "print(np.mean(is3_list))\n",
    "print(mse(is3_list))\n",
    "print(\"dm\")\n",
    "print(np.mean(dm_list))\n",
    "print(mse(dm_list))\n",
    "print(\"dr\")\n",
    "print(np.mean(dr_list))\n",
    "print(mse(dr_list))\n",
    "print(\"dr2\")\n",
    "print(np.mean(dr2_list))\n",
    "print(mse(dr2_list))\n",
    "print(\"dr3\")\n",
    "print(np.mean(dr3_list))\n",
    "print(mse(dr3_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/3000 [00:00<00:12, 234.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing SARSA to get Q prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:07<00:00, 423.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-3.3010369965309994, 4.440892098500626e-16)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Q ,episode_episode = sarsa2(env, sample_policy, sample_policy, 3000, discount_factor= gamma)\n",
    "\n",
    "gamma = gamma\n",
    "cum_reward_list = []\n",
    "for i in range(len(episode_episode)):\n",
    "    t = 0\n",
    "    cum_reward = 0\n",
    "    for episode in episode_episode[i]:\n",
    "        cum_reward += episode[-1] * gamma ** t\n",
    "        t += 1\n",
    "    cum_reward_list.append(cum_reward )   \n",
    "    # print(t)\n",
    "np.mean(cum_reward_list), np.std(cum_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
